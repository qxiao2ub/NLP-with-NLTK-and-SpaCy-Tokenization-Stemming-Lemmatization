{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8207ac28",
   "metadata": {},
   "source": [
    "# NLP with NLTK and spaCy: Tokenization, Stemming, and Lemmatization\n",
    "\n",
    "This notebook performs:\n",
    "1) Loading and preprocessing a movie review text using **NLTK** and **spaCy** (tokenization, stemming, lemmatization),\n",
    "2) Side-by-side comparison of outputs from both libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476a8650",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Run the cell below to install/download prerequisites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2450c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]\n",
      "Executable: E:\\Anaconda_new\\python.exe\n",
      "Platform: Windows-10-10.0.26100-SP0\n",
      ">> E:\\Anaconda_new\\python.exe -m pip install -U pip setuptools wheel\n",
      "\n",
      "Trying: spacy==3.7.5\n",
      ">> E:\\Anaconda_new\\python.exe -m pip install --prefer-binary --no-cache-dir spacy==3.7.5\n",
      "Imported spaCy: 3.7.5\n",
      ">> E:\\Anaconda_new\\python.exe -m spacy download en_core_web_sm\n",
      "spaCy model loaded OK.\n"
     ]
    }
   ],
   "source": [
    "import sys, platform, subprocess, os\n",
    "\n",
    "def run(cmd):\n",
    "    print(\">>\", \" \".join(cmd))\n",
    "    subprocess.run(cmd, check=False)\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Executable:\", sys.executable)\n",
    "print(\"Platform:\", platform.platform())\n",
    "\n",
    "run([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"pip\", \"setuptools\", \"wheel\"])\n",
    "\n",
    "candidates = [\n",
    "    \"spacy==3.7.5\",\n",
    "    \"spacy==3.6.1\",\n",
    "    \"spacy==3.5.4\",\n",
    "]\n",
    "for pkg in candidates:\n",
    "    print(\"\\nTrying:\", pkg)\n",
    "    run([sys.executable, \"-m\", \"pip\", \"install\", \"--prefer-binary\", \"--no-cache-dir\", pkg])\n",
    "    try:\n",
    "        import spacy\n",
    "        print(\"Imported spaCy:\", spacy.__version__)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(\"Import failed, will try next candidate...\", e)\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    import importlib\n",
    "    importlib.invalidate_caches()\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"spaCy model loaded OK.\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to load model:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f46ac27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ceea7be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47e23105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# NLTK data downloads\n",
    "def safe_nltk_download(resource):\n",
    "    try:\n",
    "        nltk.data.find(resource)\n",
    "    except LookupError:\n",
    "        nltk.download(resource.split('/')[-1], quiet=True)\n",
    "\n",
    "for res in [\n",
    "    'tokenizers/punkt',        # tokenizer\n",
    "    'taggers/averaged_perceptron_tagger',  # POS tagger\n",
    "    'taggers/averaged_perceptron_tagger_eng', # new name in some builds\n",
    "    'corpora/wordnet',\n",
    "    'corpora/omw-1.4'\n",
    "]:\n",
    "    try:\n",
    "        safe_nltk_download(res)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except Exception:\n",
    "    print(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=False)\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import pandas as pd\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d825f6f5",
   "metadata": {},
   "source": [
    "## Input Text (Movie Review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb72b15f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(767,\n",
       " 'Three years after the massive, billion-dollar success of \"Jurassic World: Dominion,\" which had brought with it the return of the original film\\'s trio of stars, it\\'s hardly a surprise to see yet another entry come about so quickly. However, while this latest sequel doesn\\'t feature Sam Neill, Laura De...')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_text = (\n",
    "    \"\"\"Three years after the massive, billion-dollar success of \\\"Jurassic World: Dominion,\\\" which had brought with it the return of the original film's trio of stars, it's hardly a surprise to see yet another entry come about so quickly. However, while this latest sequel doesn't feature Sam Neill, Laura Dern, or Jeff Goldblum, director Gareth Edwards & his team have still gone the route of adding massive star power in hopes of reinvigorating the series and keeping it fresh enough so as not to feel like just another perfunctory dinosaur outing. As we head into the seventh entry in the franchise overall, will the likes of two-time Oscar nominee Scarlett Johansson and two-time Oscar winner Mahershala Ali be enough to accomplish this extremely difficult goal?\n",
    "\n",
    "\n",
    "Starting in 2008, we witness disaster at an InGen testing site, where genetic experiments are being conducted on dinosaurs. After one of them escapes containment, it goes on a rampage and forces the rest of the personnel to abandon the site. In the present day, we learn that the remaining dinosaurs all live in areas around the equator due to it being the only climate that they're able to survive in, areas that are off-limits to everyone. Despite this, Martin Krebs (Rupert Friend), an executive for a pharmaceutical company, recruits mercenary Zora Bennett (Scarlett Johansson) and paleontologist Dr. Henry Loomis (Jonathan Bailey) for a mission to collect DNA samples from three dinosaurs in order to develop a treatment for heart disease, a very lucrative prospect. \n",
    "\n",
    "\n",
    "Before they depart, Zora recruits her old friend Duncaid Kincaid (Mahershala Ali) to lead the mission, one that immediately hits a snag when he decides to rescue Reuben Delgado (Manuel Garcia-Rulfo), his daughters Teresa (Luna Blaise) & Isabella (Audrina Miranda), and Teresa's boyfriend Xavier (David Iacono), whose boat was overturned by a dinosaur. Further trouble causes the team & those they rescued to be separated and stranded on the island where the former hopes to complete their mission, but, as one can certainly expect at this point, it turns out to be far more difficult & dangerous than originally thought.\n",
    "\n",
    "\n",
    "As one can imagine, 32 years and now seven films into this franchise, it's rather difficult for even the most talented of writers to come up with excuses for anyone to interact with the remaining dinosaurs within this universe, especially since such interactions always lead to multiple deaths. This has led screenwriter David Koepp, who adapted the first two \\\"Jurassic Park\\\" films, to fall back on one of the oldest excuses of all: simple & unbridled greed. In this case, it's a treatment for heart disease that would be worth millions, if not billions of dollars. Throw in a promised hefty payday, and it ends up being more than enough to lure a team of mercenaries into an insanely dangerous mission to procure the genetic material.\n",
    "\n",
    "\n",
    "As far as overall plots go, this one is a lot more straightforward than what we got last time. There's no potentially world-ending event or conspiracies, just a pharmaceutical company looking to make a ton of money. On that score, it may be a little more interesting, primarily because the main mission doesn't get as bogged down with a lot of superfluous material, but it still ends up being rather ho-hum because it is rather basic as far as stories go. What hurts it even more though is the completely unnecessary addition of the rescued characters from the boat, who can't help but feel superfluous because they're only there to put more people in danger while the team goes about their mission. It only becomes more apparent when you realize that they could've been eliminated entirely and it wouldn't have changed much of anything about the film.\n",
    "\n",
    "\n",
    "Again, you might be able to say that the narrative is a slight improvement, but it still doesn't do much to reinvigorate the franchise. We still have people stupidly risking their lives by going anywhere near these dinosaurs, and several people still getting killed as a result, causing it to feel like more of the same thing that we've already seen several times before. Scarlett Johansson & Mahershala Ali are certainly nice additions to the cast and do the best they can with the material they're given, but there's only so much they can do with the film's somewhat simple set-up. In the end, if you've been enjoying these films up until now, then there's a fair chance that you'll enjoy this one, but if you've come to find them a little tiresome, then this latest entry will likely do little to change your mind.\"\"\"\n",
    ")\n",
    "len(review_text.split()), review_text[:300] + '...'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5846a120",
   "metadata": {},
   "source": [
    "## Tokenization, Stemming, and Lemmatization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ddaca18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK tokens (first 40): ['Three', 'years', 'after', 'the', 'massive', ',', 'billion-dollar', 'success', 'of', '``', 'Jurassic', 'World', ':', 'Dominion', ',', \"''\", 'which', 'had', 'brought', 'with', 'it', 'the', 'return', 'of', 'the', 'original', 'film', \"'s\", 'trio', 'of', 'stars', ',', 'it', \"'s\", 'hardly', 'a', 'surprise', 'to', 'see', 'yet']\n",
      "\n",
      "NLTK Porter stems (first 40): ['three', 'year', 'after', 'the', 'massiv', ',', 'billion-dollar', 'success', 'of', '``', 'jurass', 'world', ':', 'dominion', ',', \"''\", 'which', 'had', 'brought', 'with', 'it', 'the', 'return', 'of', 'the', 'origin', 'film', \"'s\", 'trio', 'of', 'star', ',', 'it', \"'s\", 'hardli', 'a', 'surpris', 'to', 'see', 'yet']\n",
      "\n",
      "NLTK Snowball stems (first 40): ['three', 'year', 'after', 'the', 'massiv', ',', 'billion-dollar', 'success', 'of', '``', 'jurass', 'world', ':', 'dominion', ',', \"''\", 'which', 'had', 'brought', 'with', 'it', 'the', 'return', 'of', 'the', 'origin', 'film', \"'s\", 'trio', 'of', 'star', ',', 'it', \"'s\", 'hard', 'a', 'surpris', 'to', 'see', 'yet']\n",
      "\n",
      "NLTK lemmas (first 40): ['Three', 'year', 'after', 'the', 'massive', ',', 'billion-dollar', 'success', 'of', '``', 'Jurassic', 'World', ':', 'Dominion', ',', \"''\", 'which', 'have', 'bring', 'with', 'it', 'the', 'return', 'of', 'the', 'original', 'film', \"'s\", 'trio', 'of', 'star', ',', 'it', \"'s\", 'hardly', 'a', 'surprise', 'to', 'see', 'yet']\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "# Tokenization\n",
    "nltk_tokens: List[str] = word_tokenize(review_text)\n",
    "\n",
    "# Stemming\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "nltk_porter_stems = [porter.stem(tok) for tok in nltk_tokens]\n",
    "nltk_snowball_stems = [snowball.stem(tok) for tok in nltk_tokens]\n",
    "\n",
    "# Lemmatization \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def nltk_pos_to_wordnet(tag: str):\n",
    "    t = tag[0].upper()\n",
    "    if t == 'J':\n",
    "        return wordnet.ADJ\n",
    "    elif t == 'V':\n",
    "        return wordnet.VERB\n",
    "    elif t == 'N':\n",
    "        return wordnet.NOUN\n",
    "    elif t == 'R':\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "try:\n",
    "    pos_tags = pos_tag(nltk_tokens)\n",
    "except LookupError:\n",
    "\n",
    "    nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "    pos_tags = pos_tag(nltk_tokens)\n",
    "\n",
    "nltk_lemmas = [lemmatizer.lemmatize(tok, nltk_pos_to_wordnet(tag)) for tok, tag in pos_tags]\n",
    "\n",
    "print(\"NLTK tokens (first 40):\", nltk_tokens[:40])\n",
    "print(\"\\nNLTK Porter stems (first 40):\", nltk_porter_stems[:40])\n",
    "print(\"\\nNLTK Snowball stems (first 40):\", nltk_snowball_stems[:40])\n",
    "print(\"\\nNLTK lemmas (first 40):\", nltk_lemmas[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b76f58",
   "metadata": {},
   "source": [
    "## Tokenization, Stemming, and Lemmatization with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3b9dc1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy tokens (first 40): ['Three', 'years', 'after', 'the', 'massive', ',', 'billion', '-', 'dollar', 'success', 'of', '\"', 'Jurassic', 'World', ':', 'Dominion', ',', '\"', 'which', 'had', 'brought', 'with', 'it', 'the', 'return', 'of', 'the', 'original', 'film', \"'s\", 'trio', 'of', 'stars', ',', 'it', \"'s\", 'hardly', 'a', 'surprise', 'to']\n",
      "\n",
      "spaCy lemmas (first 40): ['three', 'year', 'after', 'the', 'massive', ',', 'billion', '-', 'dollar', 'success', 'of', '\"', 'Jurassic', 'World', ':', 'dominion', ',', '\"', 'which', 'have', 'bring', 'with', 'it', 'the', 'return', 'of', 'the', 'original', 'film', \"'s\", 'trio', 'of', 'star', ',', 'it', 'be', 'hardly', 'a', 'surprise', 'to']\n",
      "\n",
      "spaCy coarse POS (first 40): ['NUM', 'NOUN', 'ADP', 'DET', 'ADJ', 'PUNCT', 'NUM', 'PUNCT', 'NOUN', 'NOUN', 'ADP', 'PUNCT', 'PROPN', 'PROPN', 'PUNCT', 'NOUN', 'PUNCT', 'PUNCT', 'PRON', 'AUX', 'VERB', 'ADP', 'PRON', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'PART', 'NOUN', 'ADP', 'NOUN', 'PUNCT', 'PRON', 'AUX', 'ADV', 'DET', 'NOUN', 'PART']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(review_text)\n",
    "spacy_tokens = [t.text for t in doc]\n",
    "\n",
    "spacy_lemmas = [t.lemma_ for t in doc]\n",
    "spacy_pos = [t.pos_ for t in doc]\n",
    "\n",
    "print(\"spaCy tokens (first 40):\", spacy_tokens[:40])\n",
    "print(\"\\nspaCy lemmas (first 40):\", spacy_lemmas[:40])\n",
    "print(\"\\nspaCy coarse POS (first 40):\", spacy_pos[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8822bc5b",
   "metadata": {},
   "source": [
    "## Side-by-side Comparison\n",
    "Below we align the first 100 **non-space** tokens produced by both libraries and compare:\n",
    "- the original token text,\n",
    "- NLTK stems (Porter and Snowball),\n",
    "- NLTK lemma (with POS-aware lemmatization), and\n",
    "- spaCy lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdc9cf16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>nltk_token</th>\n",
       "      <th>nltk_porter</th>\n",
       "      <th>nltk_snowball</th>\n",
       "      <th>nltk_lemma</th>\n",
       "      <th>spacy_token</th>\n",
       "      <th>spacy_lemma</th>\n",
       "      <th>spacy_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Three</td>\n",
       "      <td>three</td>\n",
       "      <td>three</td>\n",
       "      <td>Three</td>\n",
       "      <td>Three</td>\n",
       "      <td>three</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>years</td>\n",
       "      <td>year</td>\n",
       "      <td>year</td>\n",
       "      <td>year</td>\n",
       "      <td>years</td>\n",
       "      <td>year</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>after</td>\n",
       "      <td>after</td>\n",
       "      <td>after</td>\n",
       "      <td>after</td>\n",
       "      <td>after</td>\n",
       "      <td>after</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>massive</td>\n",
       "      <td>massiv</td>\n",
       "      <td>massiv</td>\n",
       "      <td>massive</td>\n",
       "      <td>massive</td>\n",
       "      <td>massive</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>billion-dollar</td>\n",
       "      <td>billion-dollar</td>\n",
       "      <td>billion-dollar</td>\n",
       "      <td>billion-dollar</td>\n",
       "      <td>billion</td>\n",
       "      <td>billion</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>success</td>\n",
       "      <td>success</td>\n",
       "      <td>success</td>\n",
       "      <td>success</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>dollar</td>\n",
       "      <td>dollar</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>``</td>\n",
       "      <td>``</td>\n",
       "      <td>``</td>\n",
       "      <td>``</td>\n",
       "      <td>success</td>\n",
       "      <td>success</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Jurassic</td>\n",
       "      <td>jurass</td>\n",
       "      <td>jurass</td>\n",
       "      <td>Jurassic</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>World</td>\n",
       "      <td>world</td>\n",
       "      <td>world</td>\n",
       "      <td>World</td>\n",
       "      <td>\"</td>\n",
       "      <td>\"</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>Jurassic</td>\n",
       "      <td>Jurassic</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Dominion</td>\n",
       "      <td>dominion</td>\n",
       "      <td>dominion</td>\n",
       "      <td>Dominion</td>\n",
       "      <td>World</td>\n",
       "      <td>World</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>''</td>\n",
       "      <td>''</td>\n",
       "      <td>''</td>\n",
       "      <td>''</td>\n",
       "      <td>Dominion</td>\n",
       "      <td>dominion</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>which</td>\n",
       "      <td>which</td>\n",
       "      <td>which</td>\n",
       "      <td>which</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>had</td>\n",
       "      <td>had</td>\n",
       "      <td>had</td>\n",
       "      <td>have</td>\n",
       "      <td>\"</td>\n",
       "      <td>\"</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>brought</td>\n",
       "      <td>brought</td>\n",
       "      <td>brought</td>\n",
       "      <td>bring</td>\n",
       "      <td>which</td>\n",
       "      <td>which</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>with</td>\n",
       "      <td>with</td>\n",
       "      <td>with</td>\n",
       "      <td>with</td>\n",
       "      <td>had</td>\n",
       "      <td>have</td>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    idx      nltk_token     nltk_porter   nltk_snowball      nltk_lemma  \\\n",
       "0     0           Three           three           three           Three   \n",
       "1     1           years            year            year            year   \n",
       "2     2           after           after           after           after   \n",
       "3     3             the             the             the             the   \n",
       "4     4         massive          massiv          massiv         massive   \n",
       "5     5               ,               ,               ,               ,   \n",
       "6     6  billion-dollar  billion-dollar  billion-dollar  billion-dollar   \n",
       "7     7         success         success         success         success   \n",
       "8     8              of              of              of              of   \n",
       "9     9              ``              ``              ``              ``   \n",
       "10   10        Jurassic          jurass          jurass        Jurassic   \n",
       "11   11           World           world           world           World   \n",
       "12   12               :               :               :               :   \n",
       "13   13        Dominion        dominion        dominion        Dominion   \n",
       "14   14               ,               ,               ,               ,   \n",
       "15   15              ''              ''              ''              ''   \n",
       "16   16           which           which           which           which   \n",
       "17   17             had             had             had            have   \n",
       "18   18         brought         brought         brought           bring   \n",
       "19   19            with            with            with            with   \n",
       "\n",
       "   spacy_token spacy_lemma spacy_pos  \n",
       "0        Three       three       NUM  \n",
       "1        years        year      NOUN  \n",
       "2        after       after       ADP  \n",
       "3          the         the       DET  \n",
       "4      massive     massive       ADJ  \n",
       "5            ,           ,     PUNCT  \n",
       "6      billion     billion       NUM  \n",
       "7            -           -     PUNCT  \n",
       "8       dollar      dollar      NOUN  \n",
       "9      success     success      NOUN  \n",
       "10          of          of       ADP  \n",
       "11           \"           \"     PUNCT  \n",
       "12    Jurassic    Jurassic     PROPN  \n",
       "13       World       World     PROPN  \n",
       "14           :           :     PUNCT  \n",
       "15    Dominion    dominion      NOUN  \n",
       "16           ,           ,     PUNCT  \n",
       "17           \"           \"     PUNCT  \n",
       "18       which       which      PRON  \n",
       "19         had        have       AUX  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "spacy_not_space = [t for t in doc if not t.is_space]\n",
    "limit = 100\n",
    "rows = []\n",
    "for i in range(min(limit, len(nltk_tokens), len(spacy_not_space))):\n",
    "    ntok = nltk_tokens[i]\n",
    "    stok = spacy_not_space[i]\n",
    "    rows.append({\n",
    "        \"idx\": i,\n",
    "        \"nltk_token\": ntok,\n",
    "        \"nltk_porter\": porter.stem(ntok),\n",
    "        \"nltk_snowball\": snowball.stem(ntok),\n",
    "        \"nltk_lemma\": nltk_lemmas[i] if i < len(nltk_lemmas) else None,\n",
    "        \"spacy_token\": stok.text,\n",
    "        \"spacy_lemma\": stok.lemma_,\n",
    "        \"spacy_pos\": stok.pos_\n",
    "    })\n",
    "\n",
    "cmp_df = pd.DataFrame(rows)\n",
    "cmp_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ca2cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
